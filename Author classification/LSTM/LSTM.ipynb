{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ0t7hATgXCu"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "pd.options.display.max_rows = 1000\n",
        "pd.options.display.max_columns = 1000\n",
        "import json\n",
        "from tqdm import tnrange\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQBp17BZhCd8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGeJhtbcgcJ9"
      },
      "source": [
        "with open('/content/gdrive/My Drive/dataset/Line/aozora_authors.json', 'r') as myfile:\n",
        "    data = myfile.read()\n",
        "authors = json.loads(data)\n",
        "\n",
        "with open('/content/gdrive/My Drive/dataset/Line/aozora_titles.json', 'r') as myfile:\n",
        "    data = myfile.read()\n",
        "titles = json.loads(data)\n",
        "\n",
        "with open('/content/gdrive/My Drive/dataset/Line/aozora_contents.json', 'r') as myfile:\n",
        "    data = myfile.read()\n",
        "contents = json.loads(data)\n",
        "\n",
        "with open('/content/gdrive/My Drive/dataset/Line/train_aozora_data.json', 'r') as myfile:\n",
        "    data = myfile.read()\n",
        "train_data = json.loads(data)\n",
        "\n",
        "with open('/content/gdrive/My Drive/dataset/Line/train_q.json', 'r') as myfile:\n",
        "    data = myfile.read()\n",
        "train_q = json.loads(data)\n",
        "\n",
        "with open('/content/gdrive/My Drive/dataset/Line/test_aozora_data.json', 'r') as myfile:\n",
        "    data = myfile.read()\n",
        "test_data = json.loads(data)\n",
        "\n",
        "with open('/content/gdrive/My Drive/dataset/Line/test_q.json', 'r') as myfile:\n",
        "    data = myfile.read()\n",
        "test_q = json.loads(data)\n",
        "\n",
        "with open('/content/gdrive/My Drive/dataset/Line/suggestion.json.sample', 'r') as myfile:\n",
        "    data = myfile.read()\n",
        "suggestion = json.loads(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDx07TTBhXBt"
      },
      "source": [
        "print(len(authors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9oWMLVgiCsZ"
      },
      "source": [
        "print((train_data[1]['novels'][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyRB61XHlCZ0"
      },
      "source": [
        "print(contents[train_data[1]['novels'][0]['content_id']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq9BscKCjuCK"
      },
      "source": [
        "author_list = []\n",
        "title_list = []\n",
        "content_list = []\n",
        "for i in range(len(train_data)):\n",
        "  for j in range(len(train_data[i]['novels'])):\n",
        "    if train_data[i]['author_id'] == 'auth_0610':\n",
        "      print(contents[train_data[i]['novels'][j]['content_id']])\n",
        "    content_item = contents[train_data[i]['novels'][j]['content_id']]\n",
        "    content_item = content_item.replace('\\n', '')\n",
        "    content_item = content_item.replace('\\r', '')\n",
        "    if len(content_item) != 0:\n",
        "      author_list.append(train_data[i]['author_id'])\n",
        "      title_list.append(titles[train_data[i]['novels'][j]['title_id']])\n",
        "      content_list.append(content_item)\n",
        "\n",
        "train_df = pd.DataFrame(list(zip(author_list, title_list, content_list)), columns=[\"author\", \"title\", \"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL4kpE4rLA93"
      },
      "source": [
        "print('auth_0113' in train_df['author'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8d9cuGomCkX"
      },
      "source": [
        "print((train_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BkAqQv5mVpf"
      },
      "source": [
        "author_list = []\n",
        "content_list = []\n",
        "for i in tnrange(len(train_df)):\n",
        "  for j in range(0, len(train_df[\"content\"][i]), 256):\n",
        "    author_list.append(train_df['author'][i])\n",
        "    content_list.append(train_df['content'][i][j:j+256])\n",
        "decompose_256 = pd.DataFrame(list(zip(author_list, content_list)), columns=[\"author\", \"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS0cUkv3yGPW"
      },
      "source": [
        "print(len(decompose_256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNdvq2hp1U9M"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(decompose_256['author'].values)\n",
        "y = le.transform(decompose_256['author'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyeWtBfm1ko0"
      },
      "source": [
        "print(y)\n",
        "print(le.inverse_transform(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zo5lKeZKtza"
      },
      "source": [
        "print('auth_0610' in le.inverse_transform(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6zXQqcA72aD"
      },
      "source": [
        "y = pd.get_dummies(y).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeXn_TxS8zyv"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcHVkc3YyYRk"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(decompose_256['content'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvQDN7pPzxE4"
      },
      "source": [
        "print(len(tokenizer.word_counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF3RajMs0zm-"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X = tokenizer.texts_to_sequences(decompose_256['content'].values)\n",
        "X = pad_sequences(X, maxlen=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AefVWZVy2J4U"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmlzzZNG5AMD"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EaJ6amN3cs3"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, BatchNormalization,TimeDistributed\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "drop_rate = 0.1\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_counts), 128, input_length=X_train.shape[1]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(SpatialDropout1D(drop_rate))\n",
        "model.add(LSTM(256, dropout=drop_rate, recurrent_dropout=drop_rate))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(drop_rate))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryAsD6wVZMuc"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLNGAxogZOlc"
      },
      "source": [
        "files.download('model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ5ARGWv61j3"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qajgf3lt46-g"
      },
      "source": [
        "epochs = 14\n",
        "batch_size = 256\n",
        "es = EarlyStopping(monitor='val_loss', patience=5, mode='auto', restore_best_weights=True)\n",
        "for i in range(4):\n",
        "  history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data = [X_val, y_val], callbacks=[es], verbose=1)\n",
        "  model.save('line_coding_test_model_epoch_' + str(epochs) +'.h5')\n",
        "  files.download('line_coding_test_model_epoch_' + str(epochs) +'.h5')\n",
        "  epochs += 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzvfFbUGpDZY"
      },
      "source": [
        "model.save('line_coding_test_model_epoch_15.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UUyMJaKDuwG"
      },
      "source": [
        "files.download('line_coding_test_model_epoch_15.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKItM9f6SYKr"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/gdrive/My Drive/dataset/Line/line_coding_test_model_epoch_15.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWBX56WhS2P5"
      },
      "source": [
        "author_list = []\n",
        "title_list = []\n",
        "content_list = []\n",
        "for i in range(len(test_q)):\n",
        "    author_list.append(\"unknown author\")\n",
        "    title_list.append(test_q[i]['title_id'])\n",
        "    content_item = contents[test_data[test_q[i]['title_id']]]\n",
        "    content_item = content_item.replace('\\n', '')\n",
        "    content_item = content_item.replace('\\r', '')\n",
        "    content_list.append(content_item)\n",
        "\n",
        "test_df = pd.DataFrame(list(zip(author_list, title_list, content_list)), columns=[\"author\", \"title\", \"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k6KIzfnS2_o"
      },
      "source": [
        "print(test_df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuZ63_qeS48S"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx9u8MGlS6nx"
      },
      "source": [
        "test_pred_result_rank = []\n",
        "for i in tnrange(len(test_df)):\n",
        "#for i in tqdm(range(2)):\n",
        "    #pred = [0] * y_train.shape[1]\n",
        "    if len(test_df[\"content\"][i]) == 0:\n",
        "        for j in range(len(test_q[i]['candidates'])):\n",
        "            candidates.append(test_q[i]['candidates'][j])\n",
        "        test_pred_result_rank.append(candidates)\n",
        "    else:\n",
        "        X_test = []\n",
        "        for j in range(0, len(test_df[\"content\"][i]), 64):\n",
        "            X_test.append(test_df[\"content\"][i][j:j+256])\n",
        "        X_test = tokenizer.texts_to_sequences(X_test)\n",
        "        X_test = pad_sequences(X_test, maxlen=256)\n",
        "        pred = model.predict(X_test)\n",
        "        pred = np.sum(pred, axis=0)\n",
        "\n",
        "        sorted_index = np.flip(np.argsort(pred),0)\n",
        "        sorted_author = []\n",
        "        sorted_author_conf = []\n",
        "        sorted_q_author = []\n",
        "        sorted_q_author_conf = []\n",
        "        candidates = []\n",
        "        for j in range(len(sorted_index)):\n",
        "            sorted_author.append(le.inverse_transform([sorted_index[j]])[0])\n",
        "            sorted_author_conf.append(pred[sorted_index[j]])\n",
        "        for j in range(len(test_q[i]['candidates'])):\n",
        "            candidates.append(test_q[i]['candidates'][j])\n",
        "        for j in range(len(sorted_author)):\n",
        "            if sorted_author[j] in candidates:\n",
        "                sorted_q_author.append(sorted_author[j])\n",
        "                sorted_q_author_conf.append(sorted_author_conf[j])\n",
        "        test_pred_result_rank.append(sorted_q_author)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBH22c3tWsIV"
      },
      "source": [
        "with open('/content/gdrive/My Drive/dataset/Line/test_q.json', 'r') as myfile:\n",
        "    data = myfile.read()\n",
        "test_q = json.loads(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmZ3zmEIXE3i"
      },
      "source": [
        "print(set(test_q[1]['candidates']))\n",
        "print(set(test_pred_result_rank[1]))\n",
        "print(set(test_q[i]['candidates'])-set(test_pred_result_rank[i]))\n",
        "print(list(set(test_q[i]['candidates'])-set(test_pred_result_rank[i]))[0])\n",
        "test_q[1]['candidates'].append(list(set(test_q[i]['candidates'])-set(test_pred_result_rank[i]))[0])\n",
        "print(test_q[1]['candidates'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcqjDEzzTkMq"
      },
      "source": [
        "type_a = 0\n",
        "type_c = 0\n",
        "for i in range(len(test_q)):\n",
        "  if (set(test_q[i]['candidates']) != set(test_pred_result_rank[i])):\n",
        "    if (len(test_q[i]['candidates']) > len(test_pred_result_rank[i])):\n",
        "      type_a += 1\n",
        "      missing_element = list(set(test_q[i]['candidates'])-set(test_pred_result_rank[i]))[0]\n",
        "      test_q[i]['candidates'] = test_pred_result_rank[i]\n",
        "      test_q[i]['candidates'].insert((len(test_q[i]['candidates'])+1)//2, missing_element)\n",
        "    elif (len(test_q[i]['candidates']) < len(test_pred_result_rank[i])):\n",
        "      type_c += 1\n",
        "      true_list = []\n",
        "      for j in range(len(test_pred_result_rank[i])):\n",
        "        if test_pred_result_rank[i][j] in test_q[i]['candidates']:\n",
        "          true_list.append(test_pred_result_rank[i][j])\n",
        "      test_q[i]['candidates'] = true_list\n",
        "  else:\n",
        "    test_q[i]['candidates'] = test_pred_result_rank[i]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqIqg3IWYWZG"
      },
      "source": [
        "with open('suggestion.json', 'w') as outfile:\n",
        "    json.dump(test_q, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imVYTb_oYczi"
      },
      "source": [
        "files.download('suggestion.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxWA-e-D5SLt"
      },
      "source": [
        "author_list = []\n",
        "title_list = []\n",
        "content_list = []\n",
        "for i in range(len(test_q)):\n",
        "  author_list.append(\"unknown author\")\n",
        "  title_list.append(test_q[i]['title_id'])\n",
        "  content_list.append(contents[test_data['title_id']])\n",
        "\n",
        "test_df = pd.DataFrame(list(zip(author_list, title_list, content_list)), columns=[\"author\", \"title\", \"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrqiKMaQ5i66"
      },
      "source": [
        "title_list = []\n",
        "content_list = []\n",
        "for i in tqdm(range(len(test_df))):\n",
        "  for j in range(0, len(test_df[\"content\"][i]), 128):\n",
        "    title_list.append(test_df['title'][i])\n",
        "    content_list.append(test_df['content'][i][j:j+256])\n",
        "decompose_test = pd.DataFrame(list(zip(author_list, content_list)), columns=[\"title\", \"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skWN1ra06xbg"
      },
      "source": [
        "X_test = tokenizer.texts_to_sequences(decompose_test['content'].values)\n",
        "X_test = pad_sequences(X_test, maxlen=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI9chYrG7BEf"
      },
      "source": [
        "model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}