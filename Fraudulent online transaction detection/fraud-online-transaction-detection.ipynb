{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade numpy pandas matplotlib seaborn scipy tensorflow keras lightgbm nltk torch torchvision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade scikit-learn pillow opencv-python node2vec graphviz gensim fastai django cython ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r /opt/conda/lib/python3.6/site-packages/lightgbm\n!git clone --recursive https://github.com/Microsoft/LightGBM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get install -y -qq libboost-all-dev","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\nmake -j$(nproc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd LightGBM/python-package/;python3 setup.py install --precompile","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n!rm -r LightGBM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sn\nsn.set()\nfrom sklearn import preprocessing\nimport gc, datetime, random\nimport lightgbm\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\npd.options.display.max_rows = 4000\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nSEED = 42\nseed_everything(SEED)\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    convert_dict = {}\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    convert_dict.update({col: np.int8})\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    convert_dict.update({col: np.int16})\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    convert_dict.update({col: np.int32})\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    convert_dict.update({col: np.int64})  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    convert_dict.update({col: np.float16})\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    convert_dict.update({col: np.float32})\n                else:\n                    convert_dict.update({col: np.float64})\n        else:\n            convert_dict.update({col: \"category\"})\n    df = df.astype(convert_dict)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape)\n\ny_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n# Drop target, fill in NaNs\ntrain = train.drop('isFraud', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def addNewFeatures(data): \n    data['uid'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n\n    data['uid2'] = data['uid'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n\n    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n\n    data['uid4'] = data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n    \n    data['D9'] = np.where(data['D9'].isna(),0,1)\n    \n    return data\n\ntrain = addNewFeatures(train)\ntest = addNewFeatures(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i_cols = ['card1','card2','card3','card5','uid','uid2','uid3', 'uid4']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n\n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n\n        train[new_col_name] = train[col].map(temp_df)\n        test[new_col_name]  = test[col].map(temp_df)\n\ntrain = train.replace(np.inf,999)\ntest = test.replace(np.inf,999)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('std')\n\ntrain=train.replace([np.inf,-np.inf],np.nan)\ntest=test.replace([np.inf,-np.inf],np.nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['TransactionAmt'] = np.log1p(train['TransactionAmt'])\ntest['TransactionAmt'] = np.log1p(test['TransactionAmt'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nuknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].fillna(uknown)\n    df[r] = df[r].fillna(uknown)\n    \n    # Check if P_emaildomain matches R_emaildomain\n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \ntrain=setDomain(train)\ntest=setDomain(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def datetime_trans(train,start_date='2017-11-30'):\n    startdate=datetime.datetime.strptime(start_date,\"%Y-%m-%d\")\n    train['TransactionDT']=train['TransactionDT'].fillna(train['TransactionDT'].mean())\n    train['date']=train['TransactionDT'].apply(lambda x : datetime.timedelta(seconds=x)+startdate)\n    train['weekday']=train['date'].apply(lambda x :x.weekday())#不适合单独使用\n    train['month']=(train['date'].dt.year-2017)*12+train['date'].dt.month\n    train['hour']=train['date'].apply(lambda x :x.hour)#可以使用\n    train['day']=(train['date'].dt.year-2017)*365+train['date'].dt.dayofyear\n    train['year_weekday']=train['date'].apply(lambda x : str(x.year)+'_'+str(x.weekday()))#有一定的偏度，但较为平坦\n    train['weekday_hour']=train['date'].apply(lambda x :str(x.weekday())+'_'+str(x.hour))#波动性质较好\ndate_col=['weekday','month','day','hour','year_weekday','weekday_hour']\ndatetime_trans(train)\ndatetime_trans(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add 'others' mark threshold=0.95 exclude 'na's\ndef add_others_mark(train,test,categ_col):\n    temp_df=pd.concat([train[[categ_col]],test[[categ_col]]])\n    series=temp_df[categ_col].value_counts(normalize=True).cumsum()\n    others_index=list(series[series>0.95].index)\n    if len(others_index)!=0:\n        train[categ_col]=train[categ_col].apply(lambda x : 'others' if x in others_index else x)\n        test[categ_col]=test[categ_col].apply(lambda x : 'others' if x in others_index else x)\n        print(f'{categ_col}:{len(others_index)} of {len(series)} feature values has been replaced to \\'others\\'')\nmail_col=['P_emaildomain', 'R_emaildomain',\n          'DeviceInfo',\n          'id_30','id_33']       \n\nfor c in mail_col:\n    add_others_mark(train,test,c)\nadd_others_mark(train,test,mail_col[3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"lastest_browser\"] = np.zeros(train.shape[0])\ntest[\"lastest_browser\"] = np.zeros(test.shape[0])\n\ndef setBrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\ntrain=setBrowser(train)\ntest=setBrowser(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def setDevice(df):\n    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n    \n    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n\n    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    df['had_id'] = 1\n    gc.collect()\n    \n    return df\n\ntrain=setDevice(train)\ntest=setDevice(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo','device_name',\n          'id_30','id_33',\n          'uid','uid2','uid3','uid4'\n         ]+date_col\n\nfor col in i_cols:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n    train[col+'_fq_enc'] = train[col].map(fq_encode)\n    test[col+'_fq_enc']  = test[col].map(fq_encode)\n\ndef set_freq_col(train,test,col):\n    prefix='_fq'\n    temp_df=pd.concat([train[[col]],test[[col]]])\n    fq=temp_df[col].value_counts(dropna=False)\n    train[col+prefix]=train[col].map(fq)\n    test[col+prefix]=test[col].map(fq)\n    \nfor c in i_cols:\n    set_freq_col(train,test,c)\n\nperiods = ['month','year_weekday','weekday_hour']\nuids = ['uid','uid2','uid3','uid4']\ndef set_uid_period(train,test,periods,uids):\n    for period in periods:\n        for col in uids:\n            new_column = col + '_' + period\n\n            temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n            temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n            fq_encode = temp_df[new_column].value_counts()\n\n            train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n            test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n\n            train[new_column] /= train[period+'_fq']\n            test[new_column]  /= test[period+'_fq']\n            \nset_uid_period(train,test,periods,uids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_too_many_null_attr(data):\n    many_null_cols = [col for col in data.columns if data[col].isnull().sum() / data.shape[0] > 0.85]\n    return many_null_cols\n\ndef get_too_many_repeated_val(data):\n    big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.85]\n    return big_top_value_cols\n\ndef get_useless_columns(data):\n    too_many_null = get_too_many_null_attr(data)\n    print(\"More than 85% null: \" + str(len(too_many_null)))\n    too_many_repeated = get_too_many_repeated_val(data)\n    print(\"More than 85% repeated value: \" + str(len(too_many_repeated)))\n    cols_to_drop = list(set(too_many_null + too_many_repeated))\n    #cols_to_drop.remove('isFraud')\n    return cols_to_drop","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_drop = get_useless_columns(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)\nprint(y_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_cols = train.select_dtypes(exclude = 'object').columns\ncategorical_cols = train.select_dtypes(include = 'object').columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(categorical_cols))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_cols[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Label Encoding\nfor f in train.columns:\n    if train[f].dtype.name =='object' or test[f].dtype.name =='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns=[\"date\"], inplace=True)\ntest.drop(columns=[\"date\"], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.fillna(-999)\ntest = test.fillna(-999)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.isnull().sum().max())\nprint(test.isnull().sum().max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(['TransactionDT'],axis = 1, inplace = True)\ntest.drop(['TransactionDT'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"isFraud\" in train.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_SPLIT = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_n_set(n, df_x, df_y):\n    num_minor_class = np.min(df_y.value_counts())\n    #minor_class = np.argmin(df_y.value_counts().tolist())\n    minor_class = 1\n    \n    minor_x = df_x.loc[df_y == minor_class]\n    minor_y = df_y.loc[df_y == minor_class]\n    major_x = df_x.loc[df_y != minor_class]\n    major_y = df_y.loc[df_y != minor_class]\n    #print(\"no. sample in minor_x: \" + str(len(minor_x.index)))\n    #print(\"no. sample in minor_y: \" + str(len(minor_y.index)))\n    #print(\"no. sample in major_x: \" + str(len(major_x.index)))\n    #print(\"no. sample in major_y: \" + str(len(major_y.index)))\n    \n    #major_x = major_x.sample(frac=1).reset_index(drop=True)\n    rand_split = np.array_split(range(len(major_x.index)), n)\n    #print(\"len of rand_split: \" + str(len(rand_split[0])))\n    \n    major_split_x = []\n    major_split_y = []\n    for i in range(n):\n        major_part_x = major_x[0:len(rand_split[i])]\n        major_part_y = major_y[0:len(rand_split[i])]\n        major_part_x = major_part_x.append(minor_x)\n        major_part_y = major_part_y.append(minor_y)\n        major_x = major_x[len(rand_split[i]):]\n        major_y = major_y[len(rand_split[i]):]\n        major_split_x.append(major_part_x)\n        major_split_y.append(major_part_y)\n    return major_split_x, major_split_y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sub, y_train_sub = split_n_set(NUM_SPLIT, train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train, y_train\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nparams = {'num_leaves': 3,\n          'min_child_weight': 0.03,\n          'feature_fraction': 0.072,\n          'bagging_fraction': 0.072,\n          'min_data_in_leaf': 179,\n          'objective': 'binary',\n          'max_depth': 2,\n          'learning_rate': 0.006,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.33*2,\n          'reg_lambda': 0.39*2,\n          'random_state': 42,\n          'is_unbalance': True,\n          'device': 'gpu',\n          'gpu_platform_id': 0,\n          'gpu_device_id': 0\n}\nmodels = []\nfor i in range(NUM_SPLIT):\n    print(\"training model \" + str(i+1))\n    X_tr, X_val, y_tr, y_val = train_test_split(train_sub[i], y_train_sub[i], test_size = 0.2, stratify = y_train_sub[i], random_state = 42)\n    models.append(lightgbm.train(params,\n                       lightgbm.Dataset(X_tr, label=y_tr),\n                       valid_sets=lightgbm.Dataset(X_val, label=y_val),\n                       num_boost_round=10000,\n                       early_stopping_rounds=300,\n                       verbose_eval = 1000,\n                       )\n                 )\n    \ndel train_sub, y_train_sub\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ny_pred = np.zeros(len(test.index))\nfor i in range(NUM_SPLIT):\n    print(i+1)\n    y_pred += models[0].predict(test)\n    del models[0]\n    gc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.true_divide(y_pred, NUM_SPLIT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\nsubmission['isFraud'] = y_pred\nsubmission.head()\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\ncreate_download_link(filename='submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(y_pred)):\n    if y_pred[i] <= 0.5:\n        y_pred[i] = 0\n    else:\n        y_pred[i] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\nsubmission['isFraud'] = y_pred\nsubmission.head()\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_download_link(filename='submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'num_leaves': 546,\n          'min_child_weight': 0.03,\n          'feature_fraction': 0.18,\n          'bagging_fraction': 0.22,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.06,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.33,\n          'reg_lambda': 0.39,\n          'random_state': 42,\n          'is_unbalance': True\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr, X_val, y_tr, y_val = train_test_split(train, y_train, test_size = 0.2, stratify = y_train, random_state = 42)\ngc.collect()\nmodel = lightgbm.train(params,\n                       lightgbm.Dataset(X_tr, label=y_tr),\n                       valid_sets=lightgbm.Dataset(X_val, label=y_val),\n                       num_boost_round=10000,\n                       early_stopping_rounds=300,\n                       verbose_eval = 200,\n                       )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_tr, X_val, y_tr, y_val\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(\"Training Set:\")\ny_pred = model.predict(X_tr)\nfor i in range(len(y_pred)):\n    if y_pred[i] <= 0.5:\n        y_pred[i] = 0\n    else:\n        y_pred[i] = 1\nprint(classification_report(y_tr, y_pred))\n\nprint(\"Validation Set:\")\ny_pred = model.predict(X_val)\nfor i in range(len(y_pred)):\n    if y_pred[i] <= 0.5:\n        y_pred[i] = 0\n    else:\n        y_pred[i] = 1\nprint(classification_report(y_val, y_pred))\n\nprint(\"Test Set:\")\ny_pred = model.predict(X_test)\nfor i in range(len(y_pred)):\n    if y_pred[i] <= 0.5:\n        y_pred[i] = 0\n    else:\n        y_pred[i] = 1\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"intermediate_output_test = intermediate_layer_model.predict(test)\ncolumns = []\nfor i in range(intermediate_output_test.shape[1]):\n    columns.append(\"NN_feature_\" + str(i))\nnew_features_test = pd.DataFrame(intermediate_output_test, columns=columns, index=test.index)\ndel intermediate_output_test\ngc.collect()\nresult_test = pd.concat([test, new_features_test], axis=1, sort=False)\ndel new_features_test\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(result_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\nsubmission['isFraud'] = y_pred\nsubmission.head()\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\ncreate_download_link(filename='submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nresult_columns = test.columns.tolist()\nfor i in range(len(test.columns)):\n    result_columns.append(\"NN_feature_\" + str(i))\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(model.feature_importance()[:400],result_columns[:400])), columns=['Value','Feature'])\n\nplt.figure(figsize=(200, 100))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}