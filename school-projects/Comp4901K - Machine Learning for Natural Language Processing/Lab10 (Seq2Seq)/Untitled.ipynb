{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "encoder_input_train.shape (12244, 10)\n",
      "decoder_input_train.shape (12244, 10)\n",
      "Vocab Size 3243\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 10, 100)      324300      input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 10, 100)      0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 10, 200)      120600      dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 10, 200)      0           bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional [(None, 10, 200), (N 180600      dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 10, 100)      0           embedding_8[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 200)          0           bidirectional_16[0][1]           \n",
      "                                                                 bidirectional_16[0][2]           \n",
      "__________________________________________________________________________________________________\n",
      "gru_24 (GRU)                    (None, 10, 200)      180600      dropout_24[0][0]                 \n",
      "                                                                 concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_7 (Dot)                     (None, 10, 10)       0           gru_24[0][0]                     \n",
      "                                                                 bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 10, 10)       0           dot_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_8 (Dot)                     (None, 10, 200)      0           activation_5[0][0]               \n",
      "                                                                 bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 10, 400)      0           gru_24[0][0]                     \n",
      "                                                                 dot_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 10, 3243)     1300443     concatenate_12[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,106,543\n",
      "Trainable params: 2,106,543\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Traning Model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: GPU sync failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-40cd005e7f68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    234\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'posix'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                         callbacks=[TestCallback((encoder_input_valid, decoder_input_valid, decoder_target_valid), model, vocabulary)])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2696\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2697\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_make_callable_from_options'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2698\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2699\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;31m# not already marked as initialized.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 is_initialized = session.run(\n\u001b[1;32m--> 199\u001b[1;33m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: GPU sync failed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional, Input, Dense, Activation, Embedding, Dropout, TimeDistributed, GRU, Add, Lambda\n",
    "from keras.layers import dot, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from data_helper import load_data\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']=\"\" # uncomment this line, if you use cpu\n",
    "\n",
    "# training parameters\n",
    "dropout_rate = 0.2\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "embedding_dim = 100\n",
    "\n",
    "# gru parameters\n",
    "hidden_dim = 100\n",
    "num_encoder_layer = 2\n",
    "num_decoder_layer = 1\n",
    "\n",
    "# attention parameters\n",
    "attention_type = 'dot'  # [None, 'dot', 'multiplicative', 'additive']\n",
    "\n",
    "\n",
    "def seq2seq_predict(seq2seq_model, encoder_input, decoder_sequence_length, sos_idx):\n",
    "    # because we do not have the truth decoder input,\n",
    "    # we need to use the decoder prediction as its input\n",
    "    decoder_input = np.zeros(\n",
    "        shape=(len(encoder_input), decoder_sequence_length))\n",
    "    decoder_input[:, 0] = sos_idx\n",
    "    for i in range(1, decoder_sequence_length):\n",
    "        output = seq2seq_model.predict(\n",
    "            [encoder_input, decoder_input], batch_size=batch_size).argmax(axis=2)\n",
    "        decoder_input[:, i] = output[:, i]\n",
    "    decoder_output = decoder_input\n",
    "    return decoder_output\n",
    "\n",
    "\n",
    "def recover_sentence(x, idx2word):\n",
    "    s = []\n",
    "    for idx in x:\n",
    "        word = idx2word[idx]\n",
    "        if word == '<sos>':\n",
    "            continue\n",
    "        elif word == '<eos>':\n",
    "            break\n",
    "        elif word == '<pad>':\n",
    "            break\n",
    "        s.append(word)\n",
    "    return s\n",
    "\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    \"\"\"\n",
    "    Calculate BLEU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, test_data, model, vocabulary):\n",
    "        self.test_data = test_data\n",
    "        self.model = model\n",
    "        self.vocabulary = vocabulary\n",
    "        self.idx2word = dict()\n",
    "        for k, v in self.vocabulary.items():\n",
    "            self.idx2word[v] = k\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        [encoder_input, decoder_input, decoder_target] = self.test_data\n",
    "        decoder_output = seq2seq_predict(\n",
    "            self.model, encoder_input, decoder_input_train.shape[1], vocabulary['<sos>'])\n",
    "        bleu, results = self.evaluate_bleu(decoder_target, decoder_output)\n",
    "        results.sort(reverse=True)\n",
    "        print('Validation Set BLEU: %f' % (bleu))\n",
    "        print('Top | BLEU | %s | %s' %\n",
    "              ('target'.ljust(20), 'output'.ljust(20)))\n",
    "        indices = list(range(len(results)))\n",
    "        candidate_indices = list()\n",
    "        candidate_indices.extend(indices[0:3])\n",
    "        step = len(indices)//10\n",
    "        if step > 0:\n",
    "            candidate_indices.extend(indices[2+step::step])\n",
    "        if indices[-1] != candidate_indices[-1]:\n",
    "            candidate_indices.append(indices[-1])\n",
    "        for i in candidate_indices:\n",
    "            r = results[i]\n",
    "            print('%-4d|%.4f| %s | %s' %\n",
    "                  (i, r[0], ' '.join(r[1]), ' '.join(r[2])))\n",
    "\n",
    "    def evaluate_bleu(self, target, output):\n",
    "        N = target.shape[0]\n",
    "        sum_bleu = 0.0\n",
    "        results = []\n",
    "        for i in range(N):\n",
    "            t = recover_sentence(target[i], self.idx2word)\n",
    "            o = recover_sentence(output[i], self.idx2word)\n",
    "            bleu = nltk.translate.bleu_score.sentence_bleu([t], o)\n",
    "            sum_bleu += bleu\n",
    "            results.append((bleu, t, o))\n",
    "        return sum_bleu / N, results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Loading data')\n",
    "    encoder_input_train, decoder_input_train, decoder_target_train, \\\n",
    "        encoder_input_valid, decoder_input_valid, decoder_target_valid, vocabulary = load_data(\n",
    "            'translation')\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    print('encoder_input_train.shape', encoder_input_train.shape)\n",
    "    print('decoder_input_train.shape', decoder_input_train.shape)\n",
    "    print('Vocab Size', vocab_size)\n",
    "\n",
    "    num_training_data = encoder_input_train.shape[0]\n",
    "    encoder_sequence_length = encoder_input_train.shape[1]\n",
    "    decoder_sequence_length = decoder_input_train.shape[1]\n",
    "\n",
    "    # encoder_input -> [batch_size, encoder_sequence_length]\n",
    "    # decoder_input -> [batch_size, decoder_sequence_length]\n",
    "    encoder_input = Input(shape=(encoder_sequence_length,), dtype='int32')\n",
    "    decoder_input = Input(shape=(decoder_sequence_length,), dtype='int32')\n",
    "\n",
    "    # the encoder and decoder share the same embedding layer\n",
    "    emb_layer = Embedding(input_dim=vocab_size,\n",
    "                          output_dim=embedding_dim, mask_zero=True)\n",
    "\n",
    "    ################\n",
    "    # ENCODER PART #\n",
    "    ################\n",
    "\n",
    "    # embedding -> [batch_size, sequence_length, embedding_dim]\n",
    "    ### YOUR CODE HERE ###\n",
    "    encoder_input_embed = emb_layer(encoder_input)\n",
    "\n",
    "    # dropout at embedding layer\n",
    "    ### YOUR CODE HERE ###\n",
    "    encoder_input_droped = Dropout(dropout_rate)(encoder_input_embed)\n",
    "\n",
    "    # add multiple Bidirectional GRU layers here,\n",
    "    # set units=hidden_dim, return_sequences=True at the previous layers\n",
    "    # set units=hidden_dim, return_sequences=True, return_state=True at the last layer\n",
    "    # please read https://keras.io/layers/recurrent/\n",
    "    # output:\n",
    "    #     if return_sequences==True:\n",
    "    #         gru_output -> [batch_size, sequence_length, 2*hidden_dim]\n",
    "    #     if return_sequences==True and return_state=True:\n",
    "    #         gru_output -> [batch_size, sequence_length, 2*hidden_dim], [batch_size, hidden_dim], [batch_size, hidden_dim]\n",
    "\n",
    "    # N − 1 layer(s) of Bidirectional GRU, which return(s) sequences only.\n",
    "    # Dropout layers between GRU layers if applicable.\n",
    "    encoder_inputs = [encoder_input_droped]\n",
    "    for i in range(0, num_encoder_layer-1):\n",
    "        ### YOUR CODE HERE ###\n",
    "        encoder_output = Bidirectional(GRU(units = hidden_dim, return_sequences = True, unroll = True))(encoder_inputs[-1])\n",
    "        encoder_output_droped = Dropout(dropout_rate)(encoder_output)\n",
    "        encoder_inputs.append(encoder_output_droped)\n",
    "    # 1 layer of Bidirectional GRU, which returns sequences and the last state.\n",
    "    encoder_output, encoder_last_h, encoder_last_hr = Bidirectional(GRU(units=hidden_dim,\n",
    "                                                                        return_sequences=True, \n",
    "                                                                        return_state=True, \n",
    "                                                                        unroll=True))(encoder_inputs[-1])\n",
    "\n",
    "    ################\n",
    "    # DECODER PART #\n",
    "    ################\n",
    "\n",
    "    # embedding -> [batch_size, sequence_length, embedding_dim]\n",
    "    ### YOUR CODE HERE ###\n",
    "    decoder_input_embed = emb_layer(decoder_input)\n",
    "\n",
    "    # dropout at embedding layer\n",
    "    ### YOUR CODE HERE ###\n",
    "    decoder_input_droped = Dropout(dropout_rate)(decoder_input_embed)\n",
    "\n",
    "    # add multiple Unidirectional GRU layers here,\n",
    "    # set units=2*hidden_dim, return_sequences=True\n",
    "    # set initial_state=encoder_hidden_state at the first layer\n",
    "    # please read https://keras.io/layers/recurrent/\n",
    "    # output:\n",
    "    # gru_output -> [batch_size, sequence_length, 2*hidden_dim]\n",
    "\n",
    "    # 1 layer of Bidirectional GRU, whose input is the output of the encoder’s last layer\n",
    "    # and initial hidden state is the encoder’s last state.\n",
    "    ### YOUR CODE HERE ###\n",
    "    decoder_output = GRU(units = hidden_dim * 2, return_sequences = True, unroll = True)(decoder_input_droped, initial_state = concatenate([encoder_last_h, encoder_last_hr], axis = 1))\n",
    "    decoder_outputs = [decoder_output]\n",
    "    # M − 1 layer(s) of Bidirectional GRU.\n",
    "    # Dropout layers between GRU layers if applicable.\n",
    "    for i in range(1, num_decoder_layer):\n",
    "        ### YOUR CODE HERE ###\n",
    "        decoder_output_droped = Dropout(dropout_rate)(decoder_outputs[-1])\n",
    "        decoder_output = GRU(units = hidden_dim * 2, return_sequences = True, unroll = True)(decoder_output_droped[-1])\n",
    "        decoder_outputs.append(decoder_output)\n",
    "\n",
    "    # simple seq2seq without any attention mechanism\n",
    "    if attention_type is None:\n",
    "        # 1 layer of Dense layer with softmax activation wrapped by TimeDistributed.\n",
    "        output = TimeDistributed(\n",
    "            Dense(units=vocab_size, activation='softmax'))(decoder_outputs[-1])\n",
    "    else:\n",
    "        # dot-product attention\n",
    "        # weight_{i,j} = softmax(\\sum_k {decoder_output_{i,k} * encoder_output_{j,k}})\n",
    "        if attention_type == 'dot':\n",
    "            ### YOUR CODE HERE ###\n",
    "            weight = Activation('softmax')(dot([decoder_output, encoder_output], axes = [2, 2]))\n",
    "        # multiplicative attention\n",
    "        # weight_{i,j} = softmax(\\sum_k {decoder_output_{i,k} * (W encoder_output_{j,k})})\n",
    "        elif attention_type == 'multiplicative':\n",
    "            ### YOUR CODE HERE ###\n",
    "            weight = None\n",
    "        # additive attention\n",
    "        # weight_{i, j} = softmax(\\sum_k {V tanh(W1 decoder_output_{i,k} + W2 encoder_output_{j,k})})\n",
    "        elif attention_type == 'additive':\n",
    "            ### YOUR CODE HERE ###\n",
    "            # You may need the help of the Lambda wrapper(https://keras.io/layers/core/#lambda)\n",
    "            weight = None\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        attention = dot([weight, encoder_output], axes=[2, 1])\n",
    "        output = TimeDistributed(Dense(units=vocab_size, activation='softmax'))(\n",
    "            concatenate([decoder_outputs[-1], attention], axis=2))\n",
    "\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
    "\n",
    "    adam = Adam()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=adam)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    print(\"Traning Model...\")\n",
    "    history = model.fit([encoder_input_train, decoder_input_train], np.expand_dims(decoder_target_train, axis=2), \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs,\n",
    "                        verbose=1 if os.name == 'posix' else 2, \n",
    "                        callbacks=[TestCallback((encoder_input_valid, decoder_input_valid, decoder_target_valid), model, vocabulary)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
