{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am training three different machine learning models to predict the author of a book from its title and contents. The three models are:\n",
    "1. Support vector machine (SVM)\n",
    "2. Convolutional Neural Network (CNN)\n",
    "3. Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, I have already crawled a corpus named Aozora Bunko (青空文庫) from the Internet (Please see crawl_aozora_bunko.ipynb). So let's just load the corpus first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'book_content': None, 'book_title': None, 'author': None}\n"
     ]
    }
   ],
   "source": [
    "corpus_dir = 'data/corpus.json'\n",
    "\n",
    "with open(corpus_dir, 'r') as file:\n",
    "    corpus = json.load(file)\n",
    "\n",
    "# Print one book to see if it is correctly loaded\n",
    "random_key = list(corpus.keys())[0]\n",
    "print(corpus[random_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, if you have not read how I crawled the corpus, this is actually expected because the key is actually the path of the url ranging from 0 to 50000. However, it is not fully distributed, i.e., some of the ids have book contents while some do not. Therefore, our first step here would be to rule out the empty ids first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "clean_corpus = {}\n",
    "for key in corpus.keys():\n",
    "    if corpus[key]['book_content'] != None or corpus[key]['book_title'] != None or corpus[key]['author'] != None:\n",
    "        # if all key are None, the id does not have content, so it is removed\n",
    "        clean_corpus[key] = corpus[key]\n",
    "\n",
    "corpus = clean_corpus\n",
    "del clean_corpus # release memory space\n",
    "\n",
    "# expect to be 4000\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'book_content': 'き 出して 洲 になって ゐる。 しかし それ は 長さ も 幅 も、 \\n\\nそれほど 大きな もので はない。 流れ はすぐ また 合して \\n\\n一 つに なって ゐる。 こっちの 岸の 方が 深く、 川の なか \\n\\nに は 大きな 石が 幾つ もあって、 小さな 淵 を 作ったり、 \\n\\n流れが 激しく 白く 泡立った りして ゐる。 底 は 見えない _ \\n\\n向う 岸に 近いと ころ は 浅く、 河床 はすべ すべの 一枚 板 \\n\\nの やうな 感じの 岩で、 従って 水 は 音 もな く 速く 流れて \\n\\nゐる。 \\n\\nぼんやり 見て ゐた私 は その 時、 その 中洲の 上に ふと \\n\\n一 つの 生き物 を 発見した。 はじめは 土塊 だと さへ 思 は \\n\\nなかった の だが、 のろのろと それが 動きだし たので、 \\n\\n\\n\\nる 害 もない。 しかし 私に は 本能 的な 生の 衝動 以上の も \\n\\nのが あると しか 思へ なかった。 活動に は ひる 前に ぢっ \\n\\nと うづく まって ゐた 姿、 急流に 無 一 一 無 三に 突っ込んで \\n\\n行った 姿、 洲の 端に つかまって ほっとして ゐた 姿、 I \\n\\nI すべて そこに は 表情が あった。 心理 さへ あった。 そ \\n\\nれらは 人間の 場合の やうに こっちに 伝 はって 来た。 明 \\n\\n確な 目的 意志に もとづ いて 行動して ゐる ものから でな \\n\\nくて は あの 感じ は 来ない。 まして や、 あの 波間に 没し \\n\\n去った 最後の 瞬間に 至って は。 そこに は 刀 折れ、 矢尽 \\n\\nきた 感じが あった。 力の 限り 戦って 来、 最後に 運命に \\n\\n従順な ものの 姿が あった。 さう いふ もの だ けが 持つ 静 \\n\\n\\n\\nかで 赤蛙に 逢った。 私 は 夢の なかで 色 を 見る とい ふこ \\n\\nと はめった にない 人間 だ。 しかし 波間に 没す る 瞬間の \\n\\nあや \\n\\n赤蛙の 黄色い 腹と 紅の 斑紋と は 妖しい ばかりに 鮮明 だ \\n\\nつた。 \\n\\n(昭和 二十 一 年 一 月) \\n\\n\\n\\n底本 ： 「現代 日本 文學 大系 TO 武田 麟太郎 • 島 木 健 \\n\\n作 • 織 田作 之 助 • 檀 一 雄 集」 筑摩 書房 \\n\\n1970 (昭和 «) 年 6 月お 日 初版 第 ー 刷 \\n\\n入力 - j.utiyam ひ \\n\\n校正 ： かとう かおり \\n\\n1998 年 8 月^日 公開 \\n\\n2 00 5年に月？^日修正 \\n\\n青空 文庫 作成 ファイル \" \\n\\nこの ファイル は、 インタ ー ネットの 図書館、 青空 文庫 \\n\\n(http://www.aozora.gr.jp/) で 作られました。 入力、 \\n\\n校正、 制作に あたった の は、 ボランティアの 皆さんで \\n\\n\\n\\n', 'book_title': '赤蛙', 'author': 'けんさくしまき'}\n"
     ]
    }
   ],
   "source": [
    "# Print one book to see if it is correctly loaded\n",
    "random_key = list(corpus.keys())[0]\n",
    "print(corpus[random_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good to see the corpus is correctly loaded, however, we can see the next obvious problem already. There are a lot of '\\n' in the book contents. This is the line break character and is almost meaningless to the machine learning models. Therefore, let's remove them together with the whitespace. Also there is a common message at the end of each book, i.e. \n",
    "\n",
    "* Some people will also remove punctuation marks, but I think they could be useful in some case so I do not.\n",
    "* Some people will remove stop words (auxilary words that appears with very high frequency but have very limited contribution to the meaning of a sentence, such as in English, is, am, are, a, an, the, etc), but I think it is very difficult to identify these words in Japanese, so I will just leave them there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_id in corpus.keys():\n",
    "    corpus[book_id]['book_content'] = corpus[book_id]['book_content'].replace('\\n', '')\n",
    "    corpus[book_id]['book_content'] = corpus[book_id]['book_content'].replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "き出して洲になってゐる。しかしそれは長さも幅も、それほど大きなものではない。流れはすぐまた合して一つになってゐる。こっちの岸の方が深く、川のなかには大きな石が幾つもあって、小さな淵を作ったり、流れが激しく白く泡立ったりしてゐる。底は見えない_向う岸に近いところは浅く、河床はすべすべの一枚板のやうな感じの岩で、従って水は音もなく速く流れてゐる。ぼんやり見てゐた私はその時、その中洲の上にふと一つの生き物を発見した。はじめは土塊だとさへ思はなかったのだが、のろのろとそれが動きだしたので、る害もない。しかし私には本能的な生の衝動以上のものがあるとしか思へなかった。活動にはひる前にぢっとうづくまってゐた姿、急流に無一一無三に突っ込んで行った姿、洲の端につかまってほっとしてゐた姿、IIすべてそこには表情があった。心理さへあった。それらは人間の場合のやうにこっちに伝はって来た。明確な目的意志にもとづいて行動してゐるものからでなくてはあの感じは来ない。ましてや、あの波間に没し去った最後の瞬間に至っては。そこには刀折れ、矢尽きた感じがあった。力の限り戦って来、最後に運命に従順なものの姿があった。さういふものだけが持つ静かで赤蛙に逢った。私は夢のなかで色を見るといふことはめったにない人間だ。しかし波間に没する瞬間のあや赤蛙の黄色い腹と紅の斑紋とは妖しいばかりに鮮明だつた。(昭和二十一年一月)底本：「現代日本文學大系TO武田麟太郎•島木健作•織田作之助•檀一雄集」筑摩書房1970(昭和«)年6月お日初版第ー刷入力-j.utiyamひ校正：かとうかおり1998年8月^日公開2005年に月？^日修正青空文庫作成ファイル\"このファイルは、インターネットの図書館、青空文庫(http://www.aozora.gr.jp/)で作られました。入力、校正、制作にあたったのは、ボランティアの皆さんで\n"
     ]
    }
   ],
   "source": [
    "# Print one book to see if line breaks are correctly removed\n",
    "random_key = list(corpus.keys())[0]\n",
    "print(corpus[random_key]['book_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start to analyse the data a bit because it is always a good habit to first look into the data before really training a model.\n",
    "Let's first look at the distribution of classes given that this is a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 122\n"
     ]
    }
   ],
   "source": [
    "author_count = {}\n",
    "for book_id in corpus.keys():\n",
    "    author = corpus[book_id]['author']\n",
    "    if author not in author_count.keys():\n",
    "        author_count[author] = 1\n",
    "    else:\n",
    "        author_count[author] += 1\n",
    "print(f'Number of authors: {len(author_count)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_count_sorted = {k.replace(\" \", \"　\"): v for k, v in sorted(author_count.items(), reverse=True, key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: 　　　　　　　　　　　　　　　　　ゆりこみやもと, Count:  397\n",
      "Author: 　　　　　　　　　　　　　りゅうのすけあくたがわ, Count:  343\n",
      "Author: 　　　　　　　　　　　　　　　　　とらひこてらだ, Count:  271\n",
      "Author: 　　　　　　　　　　　　　　　　　あんごさかぐち, Count:  269\n",
      "Author: 　　　　　　　　　　　　　　　　　よしおとよしま, Count:  208\n",
      "Author: 　　　　　　　　　　　　　　　　　　くにおきしだ, Count:  208\n",
      "Author: 　　　　　　　　　　　　　　　　　　おさむだざい, Count:  207\n",
      "Author: 　　　　　　　　　　　　　　　　　きどうおかもと, Count:  169\n",
      "Author: 　　　　　　　　　　　　　　　　　じゅうざうんの, Count:  161\n",
      "Author: 　　　　　　　　　　　　　　　　　けんじみやざわ, Count:  106\n",
      "Author: 　　　　　　　　　　　　　　　　　そうせきなつめ, Count:   86\n",
      "Author: 　　　　　　　　　　　　　　　　　しんいちまきの, Count:   85\n",
      "Author: 　　　　　　　　　　　　　　　　こうたろうたなか, Count:   84\n",
      "Author: 　　　　　　　　　　　　　　　　　　おうがいもり, Count:   78\n",
      "Author: 　　　　　　　　　　　　　　　　　きょうかいずみ, Count:   75\n",
      "Author: 　　　　　　　　　　　　　　　　　かのこおかもと, Count:   72\n",
      "Author: 　　　　　　　　　　　　　　　　きゅうさくゆめの, Count:   67\n",
      "Author: 　　　　　　　　　　　　　　　　むらさき　しきぶ, Count:   55\n",
      "Author: 　　　　　　　　　　　　　　　　　　　かんきくち, Count:   48\n",
      "Author: 　　　　　　　　　　　　　　　　　しろうくにえだ, Count:   48\n",
      "Author: 　　　　　　　　　　　　　　　　　しのぶおりくち, Count:   47\n",
      "Author: 　　　　　　　　　　　　　　　　　まさおくすやま, Count:   43\n",
      "Author: 　　　　　　　　　　　　　　　　　なんきちにいみ, Count:   42\n",
      "Author: 　　　　　　　　　　　　　　　　かいざんなかざと, Count:   41\n",
      "Author: 　　　　　　　　　　　　　　　　　　あきこよさの, Count:   39\n",
      "Author: 　　　　　　　　　　　　　　　　　　　たみきはら, Count:   37\n",
      "Author: 　　　　　　　　　　　　　　　　　りいちよこみつ, Count:   34\n",
      "Author: 　　　　　　　　　　　　　　　　　しぐれはせがわ, Count:   32\n",
      "Author: 　　　　　　　　　　　　　　　　　どっぽくにきだ, Count:   28\n",
      "Author: 　　　　　　　　　　　　　　　　　　ふみこはやし, Count:   27\n",
      "Author: 　　　　　　　　　　　　　　　　　たけおありしま, Count:   25\n",
      "Author: 　　　　　　　　　　　　　　　きゅうきんすすきだ, Count:   25\n",
      "Author: 　　　　　　　　　　　　　　　　もとじろうかじい, Count:   23\n",
      "Author: 　　　　　　　　　　　　　　　　　としろうささき, Count:   21\n",
      "Author: 　　　　　　　　　　　　　　　　　みえきちすずき, Count:   20\n",
      "Author: 　　　　　　　　　　　　　　　　　　　たつおほり, Count:   19\n",
      "Author: 　　　　　　　　　　　　　　　　たくぼくいしかわ, Count:   18\n",
      "Author: 　　　　　　　　　　　　　　　　　さくのすけおだ, Count:   17\n",
      "Author: 　　　　　　　　　　　　　　　　けいきちおおさか, Count:   17\n",
      "Author: 　　　　　　　　　　　　　　　　　　じゅんとさか, Count:   17\n",
      "Author: 　　　　　　　　　　　　　　　さくたろうはぎわら, Count:   16\n",
      "Author: 　　　　　　　　　　　　　　　　　まんさくいたみ, Count:   15\n",
      "Author: 　　　　　　　　　　　　　　　　　　ひでおおぐま, Count:   14\n",
      "Author: 　　　　　　　　　　　　　　　　えいすけよしゆき, Count:   12\n",
      "Author: 　　　　　　　　　　　　　　　　　うじょうのぐち, Count:   12\n",
      "Author: 　　　　　　　　　　　　　　　　とうこくきたむら, Count:   12\n",
      "Author: 　　　　　　　　　　　　　　　　　　さちおいとう, Count:   11\n",
      "Author: 　　　　　　　　　　　　　　　　むしたろうおぐり, Count:   11\n",
      "Author: 　　　　　　　　　　　　　　　　とうそんしまざき, Count:   11\n",
      "Author: 　　　　　　　　ハンス・クリスチャンアンデルセン, Count:   11\n",
      "Author: 　　　　　　　　　　　　　　　　　　おんわたなべ, Count:   10\n",
      "Author: 　　　　　　　　　　　　　　はつのすけひらばやし, Count:   10\n",
      "Author: 　　　　　　　　　　　　　　　　　あつしなかじま, Count:    9\n",
      "Author: 　　　　　　　　　　　　　　　　　　しきまさおか, Count:    9\n",
      "Author: 　　　　　　　　　　　　　　　はくしゅうきたはら, Count:    8\n",
      "Author: 　　　　　　　　　　　　　　　　　　　じゅんつじ, Count:    8\n",
      "Author: 　　　　　　　　　　　　　　　　　ふぼくこさかい, Count:    8\n",
      "Author: 　　　　　　　　　　　　　　　　　もきちさいとう, Count:    8\n",
      "Author: 　　　　　　　　　　　ヴィルヘルム・カールグリム, Count:    8\n",
      "Author: 　　　　　　　　　　　　　　　　じゅうらんひさお, Count:    8\n",
      "Author: 　　　　　　　　　　　　　　　　　　　しづしらき, Count:    7\n",
      "Author: 　　　　　　　　　　　　　　　　　きたろうにしだ, Count:    7\n",
      "Author: 　　　　　　　　　　　　　アーサー・コナンドイル, Count:    7\n",
      "Author: 　　　　　　　　　　　　　　エドガー・アランポー, Count:    6\n",
      "Author: 　　　　　　　　　　　　　　　　　たきじこばやし, Count:    6\n",
      "Author: 　　　　　　　　　　　　　　　　　いちようひぐち, Count:    6\n",
      "Author: 　　　　　　　　　　　　　　　　　　ふぼうはやし, Count:    6\n",
      "Author: 　　　　　　　　　　　　　　　　　　　のえいとう, Count:    6\n",
      "Author: 　　　　　　　　　　　　　　　アントンチェーホフ, Count:    6\n",
      "Author: 　　　　　　　　　　　　　　　　　　かたいたやま, Count:    5\n",
      "Author: 　　　　　　　　　　　　　　　　ひゃくぞうくらた, Count:    5\n",
      "Author: 　　　　　　　　　　　　　　　　　　ろはんこうだ, Count:    5\n",
      "Author: 　　　　　　　　　　　　　　　さんじゅうごなおき, Count:    5\n",
      "Author: 　　　　　　　　　　　　　　　　　こじんしもむら, Count:    5\n",
      "Author: 　　　　　　　　　　　　　　　　　なおえきのした, Count:    5\n",
      "Author: 　　　　　　　　　　　　　　　　　けんさくしまき, Count:    4\n",
      "Author: 　　　　　　　　　　　　　　　　　ほうさいおざき, Count:    4\n",
      "Author: 　　　　　　　　　　　　　　　　　さかえおおすぎ, Count:    4\n",
      "Author: 　　　　　　　　　　　　　　　　　ゆめじたけひさ, Count:    4\n",
      "Author: 　　　　　　　　　　　　　　　　　かいしゅうかつ, Count:    4\n",
      "Author: 　　　　　　　　　　　　　　　しょうえんうえむら, Count:    4\n",
      "Author: 　　　　　　　　　　　　　　　　　ならしげこいで, Count:    4\n",
      "Author: 　　　　　　　　　　　　　　　　ゆうゆうきりゅう, Count:    4\n",
      "Author: 　　　　　　　　　　　　　　　　　シャルルペロー, Count:    4\n",
      "Author: 　　　　　　　　　　　　　　　　しめいふたばてい, Count:    3\n",
      "Author: 　　　　　　　　　　　　　　　　　しゅうぞうくき, Count:    3\n",
      "Author: 　　　　　　　　　　　　　　　　　　　きよしみき, Count:    3\n",
      "Author: 　　　　　　　　　　　　　　　　　やくもこいずみ, Count:    3\n",
      "Author: 　　　　　　　　　　　　　　　こうたろうたかむら, Count:    3\n",
      "Author: 　　　　　　　　　　　　　　　　　じゅうきちやぎ, Count:    2\n",
      "Author: 　　　　　　　　　　　　　　　　　かいたむらやま, Count:    2\n",
      "Author: 　　　　　　　　　　　　　　　　みちぞうたちはら, Count:    2\n",
      "Author: 　　　　　　　　　　　　　　　　　ひでみつたなか, Count:    2\n",
      "Author: 　　　　　　　　　　　　　　　　るいこうくろいわ, Count:    2\n",
      "Author: 　　　　　　　　　　　　　　　　　　たくじおおて, Count:    2\n",
      "Author: 　　　　　　　　　　　　　　　　　たかしながつか, Count:    2\n",
      "Author: 　　　　　　　　　　　　　　　　モーリスルブラン, Count:    2\n",
      "Author: 　　　　　　　　　　　　　　　　　ゆきちふくざわ, Count:    2\n",
      "Author: 　　　　　　　　　　　　　　　　ちゅうやなかはら, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　　　よしきはやま, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　かんぞううちむら, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　ニコライゴーゴリ, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　　しずこわかまつ, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　かくぞうおかくら, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　ぶんたろうかとう, Count:    1\n",
      "Author: 　　フィヨードル・ミハイロヴィチドストエフスキー, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　　　　びんうえだ, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　りょくうさいとう, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　チャールズディケンズ, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　ジョナサンスウィフト, Count:    1\n",
      "Author: 　　　　　　　　　マリー・ルイーズド・ラ・ラメー, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　　さんきちとうげ, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　　さいかくいはら, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　ナサニエルホーソーン, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　なおじろうささき, Count:    1\n",
      "Author: 　　　　　　　　　　　　ルイーザ・メイオルコット, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　　くにひこすがわ, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　　　ルネデカルト, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　ギ・ドモーパッサン, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　さんとうかたねだ, Count:    1\n",
      "Author: 　　　　　　　　　　　　　　　　ぼちょうやまむら, Count:    1\n",
      "Author: 　　　　　　　　　　　　エドモンド・デアミーチス, Count:    1\n"
     ]
    }
   ],
   "source": [
    "for author in author_count_sorted.keys():\n",
    "    print(u'Author: {:\\u3000>24s}, Count: {:>4d}'.format(author, author_count_sorted[author]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution does not look good actually (and yes, I know, this is very often the case). One of the worst things is that some author only have a few books. To be honest, *each book content will become more than one training samples*<sup>1</sup> later, but I am still going to remove these author from the corpus because there will be a **data leakage**<sup>2</sup> problem.\n",
    "\n",
    "<span style=\"font-size:12px\">1: since most machine learning models take in fixed length input, and the full contents will be way too large as an input, so a common practice is to split the contents into short fragments such that they can be fed into the models<br>\n",
    "2: data leakage refers to when data with strong connection, i.e. two observation of the same datum, variants originating from a same datum, etc, are put into both training set and validation set (and even test set). This will falsely enhance the performance of a machine learning model by overfitting because what are supposed to be in validation set are leaked and present in training set.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "author_with_more_books = set()\n",
    "for author in author_count.keys():\n",
    "    if author_count[author] >= 5:\n",
    "        author_with_more_books.add(author)\n",
    "print(len(author_with_more_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_corpus = {}\n",
    "for book_id in corpus.keys():\n",
    "    if corpus[book_id]['author'] in author_with_more_books:\n",
    "        sub_corpus[book_id] = corpus[book_id]\n",
    "corpus = sub_corpus\n",
    "del sub_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3907\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed almost 50 authors but since they are authors with too few books, we only removed less than 100 books from the corpus, sound good! The next step would be to transform the corpus into some format which can be trained. But before that, we may want to spilt the corpus into training set, validaiton set and test set first so as to prevent data leakage. The goal is to assign books into train, validation and test set respectively with a ratio of approximately 3:1:1, i.e. 60% in training set, 20% in validation set and 20% in test set. Also, we want the distrubution of author, i.e. class, to be more or less the same amoung the three sets.\n",
    "\n",
    "This can be done easily with a function of sklean called train_test_split with stratify turned on. However, our corpus now has a bit different structure so let's twist it a bit first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we transform things into lists. Note we can always retrieve the title, author and contents of a book by its id so we can create the assignment by book ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_list = list(corpus.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get the authors into class in form of a number with the help of sklearn's label encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = []\n",
    "for book_id in corpus.keys():\n",
    "    author_list.append(corpus[book_id]['author'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le = le.fit(author_list)\n",
    "author_encoded_list = le.transform(author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29 68 68 ... 45 45 45]\n"
     ]
    }
   ],
   "source": [
    "print(author_encoded_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply the train_test_split function twice to split the corpus into three sets.\n",
    "\n",
    "Note that 80% X 25% = 20% so at second time the test_size is 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_train, book_id_test, author_train, author_test = train_test_split(book_id_list, author_list, test_size=0.2, stratify=author_list, random_state=42)\n",
    "book_id_train, book_id_val, author_train, author_val = train_test_split(book_id_train, author_train, test_size=0.25, stratify = author_train, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to construct the training set, validation set and test set by book_id with book author, title and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_train = []\n",
    "content_train = []\n",
    "author_train = []\n",
    "for book_id in book_id_train:\n",
    "    title_train.append(corpus[book_id]['book_title'])\n",
    "    content_train.append(corpus[book_id]['book_content'])\n",
    "    author_train.append(corpus[book_id]['author'])\n",
    "\n",
    "title_val = []\n",
    "content_val = []\n",
    "author_val = []\n",
    "for book_id in book_id_val:\n",
    "    title_val.append(corpus[book_id]['book_title'])\n",
    "    content_val.append(corpus[book_id]['book_content'])\n",
    "    author_val.append(corpus[book_id]['author'])\n",
    "    \n",
    "title_test = []\n",
    "content_test = []\n",
    "author_test = []\n",
    "for book_id in book_id_test:\n",
    "    title_test.append(corpus[book_id]['book_title'])\n",
    "    content_test.append(corpus[book_id]['book_content'])\n",
    "    author_test.append(corpus[book_id]['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's save them for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(file_dir, python_object):\n",
    "    with open(file_dir, 'wb') as file: # pickle reads and writes in binary\n",
    "        pickle.dump(python_object, file)\n",
    "        \n",
    "save_file('data/title_train.pickle', title_train)\n",
    "save_file('data/content_train.pickle', content_train)\n",
    "save_file('data/author_train.pickle', author_train)\n",
    "\n",
    "save_file('data/title_val.pickle', title_val)\n",
    "save_file('data/content_val.pickle', content_val)\n",
    "save_file('data/author_val.pickle', author_val)\n",
    "\n",
    "save_file('data/title_test.pickle', title_test)\n",
    "save_file('data/content_test.pickle', content_test)\n",
    "save_file('data/author_test.pickle', author_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
