{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to train a text classification model by tf-idf and svm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, load the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_dir):\n",
    "    with open(file_dir, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "title_train = load_file('data/title_train.pickle')\n",
    "content_train = load_file('data/content_train.pickle')\n",
    "author_train = load_file('data/author_train.pickle')\n",
    "\n",
    "title_val = load_file('data/title_val.pickle')\n",
    "content_val = load_file('data/content_val.pickle')\n",
    "author_val = load_file('data/author_val.pickle')\n",
    "\n",
    "title_test = load_file('data/title_test.pickle')\n",
    "content_test = load_file('data/content_test.pickle')\n",
    "author_test = load_file('data/author_test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use tf-idf, we will first concatenate all the book contents writen by the same author into their respective string. This is for learning the tf-idf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "author_list_train = list(set(author_train))\n",
    "concatenated_string_train = []\n",
    "for author_of_author_list in author_list_train:\n",
    "    concatenated_string = \"\"\n",
    "    for idx, author_of_author_train in enumerate(author_train):\n",
    "        if author_of_author_list == author_of_author_train:\n",
    "            concatenated_string += title_train[idx]\n",
    "            concatenated_string += content_train[idx]\n",
    "    concatenated_string_train.append(concatenated_string)\n",
    "\n",
    "# just a simple check that we have same number of author and his / her corresponding all-in-one string\n",
    "print(len(author_list_train))\n",
    "print(len(concatenated_string_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation and test set, no need to concatenate all books because in fact we are going to make prediction on each of the books. So we will just concatenate the title and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list_val = author_val\n",
    "concatenated_string_val = []\n",
    "for title, content in zip(title_val, content_val):\n",
    "    concatenated_string_val.append(title + content)\n",
    "\n",
    "author_list_test = author_test\n",
    "concatenated_string_test = []\n",
    "for title, content in zip(title_test, content_test):\n",
    "    concatenated_string_test.append(title + content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to convert the corpus into tf-idf vectors, sklearn provides a function to achieve so easily.\n",
    "\n",
    "But before that, it would be good to first know more about what tf-idf actually is. The term tf-idf is actually a combination of two components namely term frequency (tf) and inverse document frequency (idf). And before we talk about what these two components mean, we need to know a few teminologies, which are:\n",
    "\n",
    "(word) token: usually refers to a unique word. e.g. the sentence *\"How do you do\"*, although having four words, has three word tokens. But it really depends on the tokenizer used and parameters settings such as range of n-gram.<br>\n",
    "document: all the text belonging to the same class. In this project, it refers to all the book titles and contents written by the same author.<br>\n",
    "\n",
    "The general idea is:<br>\n",
    "**Term frequency**: how many times a word token appears in a document.<br>\n",
    "**Inverse document frequency**: the inverse of in how many documents a word token appears.<br>\n",
    "To be precise, their formula looks like:<br>\n",
    "<span style=\"font-size:16px;text-align:center;\">$tf = \\frac{number\\, of\\, occurence\\, of\\, a\\, term\\, in\\, the\\, document}{number\\, of\\, words\\, in\\, the\\, document}$</span> and, <span style=\"font-size:16px\">$idf = log_e({\\frac{total\\, number\\, of\\, documents}{number\\, of\\, documents\\, in\\, which\\, the\\, term\\, appears}})$</span><br>\n",
    "These two values are computed for each token of each document and are multiplied to become tf-idf, then the tf-idf values of each token in the same document are concatenated to become a tf-idf vector represnting the document.\n",
    "\n",
    "Term frequency is very straight-forward that we think of a word token as more important if it appears more often. However, it actually has a drawback that tokens with the highest term frequencies are very likely to be stopwords and words that are common thoughout every class. Giving these words high value does not help in classification a lot.<br>Therefore inverse document frequency is introduced. If a word appears very frequently in a document but it also appears in most documents, because of a smaller idf value, the tf-idf will become smaller as well. On the other hand, if a word appears frequently in a document and it only appears in this one document, then very likely this word is very important and icoinc for this class, the tf-idf can reflect this well too. Therefore, tf-idf is a very elegant and common algorithm to vectorize text in NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.8, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note of parameters\n",
    "# analyzer: char because we want each word character to be a token. Since Japanese, unlike English, does not seperate words by space naturally\n",
    "# ngram_range: we recognize tokens that appear together in same other as an unique token, the range (1,3) means we accept unigram (1), bigram (2) and trigram (3).\n",
    "# max_df: all tokens with document frequency higher than this value is given up in the vector, they usually are stopwords\n",
    "# smooth_idf: adding 1 to the document frequency when computing idf to prevent the denominator being 0. \n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1,3), max_df=0.8, )\n",
    "vectorizer.fit(concatenated_string_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(concatenated_string_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 2313293)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's do the same for validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = vectorizer.transform(concatenated_string_val)\n",
    "X_test = vectorizer.transform(concatenated_string_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have finished taking care about the X, let's do the y part as well. The author list are list of string for the moement, but we will need a list of class number. We have actually done something similar in the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le = le.fit(author_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(author_list_train)\n",
    "y_val = le.transform(author_list_val)\n",
    "y_test = le.transform(author_list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can feed the data to our machine learning model named Support Vector Machine (SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.94      0.89      0.91        54\n",
      "           3       1.00      1.00      1.00         1\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         2\n",
      "           6       1.00      0.06      0.12        16\n",
      "           7       0.91      1.00      0.95        41\n",
      "           8       1.00      0.50      0.67         2\n",
      "           9       0.62      1.00      0.76         8\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.78      0.50      0.61        14\n",
      "          12       0.00      0.00      0.00        10\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.87      0.97      0.92        34\n",
      "          15       0.75      0.60      0.67         5\n",
      "          16       0.67      1.00      0.80        14\n",
      "          17       0.75      1.00      0.86        15\n",
      "          18       0.87      0.98      0.92        42\n",
      "          19       1.00      0.75      0.86         4\n",
      "          20       0.95      0.90      0.93        21\n",
      "          21       0.94      1.00      0.97        17\n",
      "          22       0.50      1.00      0.67         1\n",
      "          23       0.00      0.00      0.00         3\n",
      "          24       0.00      0.00      0.00         4\n",
      "          25       0.00      0.00      0.00         2\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.00      0.00      0.00         2\n",
      "          28       1.00      1.00      1.00         7\n",
      "          29       0.50      1.00      0.67         1\n",
      "          30       0.91      1.00      0.95        10\n",
      "          31       0.60      0.90      0.72        10\n",
      "          32       0.35      0.94      0.51        17\n",
      "          33       0.94      1.00      0.97        32\n",
      "          34       1.00      1.00      1.00         1\n",
      "          35       1.00      1.00      1.00         1\n",
      "          36       0.67      1.00      0.80         4\n",
      "          37       0.52      0.65      0.58        17\n",
      "          38       0.00      0.00      0.00         1\n",
      "          39       1.00      0.75      0.86         4\n",
      "          40       0.00      0.00      0.00         5\n",
      "          41       0.80      1.00      0.89         4\n",
      "          42       0.78      0.88      0.82         8\n",
      "          43       0.67      1.00      0.80         2\n",
      "          44       1.00      0.50      0.67         2\n",
      "          45       1.00      0.75      0.86         4\n",
      "          46       0.87      0.98      0.92        54\n",
      "          47       1.00      0.83      0.91         6\n",
      "          48       1.00      1.00      1.00         1\n",
      "          49       1.00      0.89      0.94         9\n",
      "          50       0.00      0.00      0.00         1\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       1.00      0.50      0.67         2\n",
      "          53       1.00      0.67      0.80         3\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         1\n",
      "          56       1.00      1.00      1.00         1\n",
      "          57       1.00      0.17      0.29         6\n",
      "          58       0.73      0.89      0.80         9\n",
      "          59       1.00      1.00      1.00         3\n",
      "          60       1.00      0.75      0.86         4\n",
      "          61       1.00      1.00      1.00         2\n",
      "          62       1.00      1.00      1.00        11\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       1.00      1.00      1.00         5\n",
      "          65       0.97      0.94      0.95        79\n",
      "          66       1.00      0.93      0.96        42\n",
      "          67       1.00      0.43      0.60         7\n",
      "          68       0.94      0.86      0.89        69\n",
      "          69       0.00      0.00      0.00         1\n",
      "          70       1.00      1.00      1.00         1\n",
      "          71       1.00      1.00      1.00         1\n",
      "          72       1.00      1.00      1.00         1\n",
      "          73       1.00      1.00      1.00         2\n",
      "          74       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.85       782\n",
      "   macro avg       0.69      0.67      0.66       782\n",
      "weighted avg       0.85      0.85      0.83       782\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\python3.7\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
